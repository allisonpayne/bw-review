# Beaked whale systematic review

Our first step is to build the query that we want to use. To make sure we are choosing keywords in a less biased way, we want to use the following procedure:

1.  Download the 10 most cited beaked whale papers from Web of Science with their abstracts.
2.  Use `tidytext` to do a simple text analysis of the abstracts, enabling us to find common word pairs.
3.  Manually review this list of words to see what should be included in our query.

To do this, I followed the excellent tutorials on [tidytextmining.com](https://www.tidytextmining.com/)!

First, setup and reading in the data.

```{r}

library(tidytext)
library(tidyverse)
library(ggplot2)
library(tidyr)

top_ten <- read.csv(here::here("data/top_ten.csv"))
data("stop_words")
```

Next, we split apart each abstract such that each word has its own row. We also remove common "stop words", such as "the" or "and".

```{r}
all_abs <- top_ten %>% 
  unnest_tokens(word, Abstract) %>% 
  anti_join(stop_words) %>% 
  select(Article.Title, Source.Title, Authors, Publication.Year, DOI, word)
head(all_abs)
```

Count and plot the most frequently used words.

```{r}
all_abs %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 5) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(n, word)) + 
  geom_col() +
  labs(y = NULL)
```

Next, we want to explore the relationship between words. We will use the `token = "ngrams"` argument to tokenize pairs of adjacent words, rather than individual ones. Because we want pairs of words, they're called "bigrams".

```{r}
bigram_abs <- top_ten %>% 
  unnest_tokens(bigram, Abstract, token = "ngrams", n = 2) %>% 
  filter(!is.na(bigram)) %>% 
  select(Article.Title, Source.Title, Authors, Publication.Year, DOI, bigram)
head(bigram_abs)
```

However, this includes all the "stop words" that we wanted to exclude. Let's remove them.

```{r}
bigrams_sep <- bigram_abs %>% 
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filter <- bigrams_sep %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)

head(bigrams_filter)
```

Now let's count again.

```{r}
bigram_counts <- bigrams_filter %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 2) 

bigram_counts
```

This gives us our most common word pairs.
